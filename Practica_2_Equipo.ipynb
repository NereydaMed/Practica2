{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practica_2_Equipo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNu+kA5toS5wTBkSUudzV5h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NereydaMed/Practica2/blob/master/Practica_2_Equipo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EIfiV6a7KpM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ce24feb8-0d4c-46d5-ae24-5bf8dd6bc30f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import numpy as np    \n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "from textblob import TextBlob"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8WpIcWqLXiL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Textos Jaaziel\n",
        "#Párrafo 1\n",
        "text_01 = \"Hola, hola, hola hola\"\n",
        "\n",
        "#Párrafo 2\n",
        "text_02 = \"Hi, this class is very interesting, I like to program, but I'm not very good yet.\"\n",
        "\n",
        "#Párrafo 3\n",
        "text_03 = \"Γεια σου, αυτή η τάξη είναι πολύ ενδιαφέρουσα, μου αρέσει να προγραμματίζω, αλλά δεν είμαι πολύ καλός ακόμα.\"\n",
        "\n",
        "# https://twitter.com/GaeboraKae/status/1169424976413372416\n",
        "text_04 = \"こんにちは、このクラスは非常に興味深いです、私はプログラミングが好きですが、私はまだあまりよくありません\"\n",
        "\n",
        "# https://twitter.com/VICEenEspanol/status/1169705966234943488\n",
        "text_05 = \"Привет, этот класс очень интересный, я люблю программировать, но я еще не очень хорош.\"\n",
        "\n",
        "# https://twitter.com/Anaro74/status/1169579828963463168\n",
        "text_06 = \"Salve, hoc genus est valde interesting, ego similis programming, sed tamen Im 'non valde bona.\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1g4GNzrMs3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Textos Deni\n",
        "#Párrafo 1\n",
        "text_07 = \"¡Se une al #DíaSinMujeres! La cadena estadounidense Hooters publicó que contará con servicio de comida para llevar y servicio a domicilio, este último mediante Uber Eats\"\n",
        "\n",
        "#Párrafo 2\n",
        "text_08 = \"En vez de darles el día deberían dejar de cosificar a sus empleadas.\"\n",
        "\n",
        "#Párrafo 3\n",
        "text_09 = \"Contradictorio que apoye el paro de mujeres Rostro pensativo pero pida que su personal femenino utilice esos mini shorts y blusas scotadas por ahí debería empezar su cambio \"\n",
        "\n",
        "#https://twitter.com/sopitas/status/1236098576238997504 \n",
        "text_10 = \"La Selección Femenil de futbol le ganó a Haití y obtuvo su pase al mundial ¡Muchas felicidades! Poniendo el futbol de nuestro país en alto\"\n",
        "\n",
        "#https://twitter.com/meelycampos/status/1233640560444854272\n",
        "text_11 = \"Tengo 1 año 5 meses con mi novio y nunca ha querido subir una foto conmigo a ninguna red social. Cada like es un amiga date cuenta.\"\n",
        "\n",
        "#https://twitter.com/eileenOrnot/status/1233435752937246720\n",
        "text_12 = \"La tasa de mortalidad por feminicidio creció 138%. La del coronavirus es del 2%. O sea que si hoy en la noche te besuquearas a un guey infectado tienes más posibilidades de morir asesinada por el tipo que por el virus. Buenos días, República Mexicana.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcAnKwOz7q-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_tw = [text_01,text_02,text_03,text_04,text_05,text_06,text_07,text_08,text_09,text_10,text_11,text_12]\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ypk9C0m_77-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def token_bow_mtdb(x):\n",
        "    text_tokens = []\n",
        "    text_tokens_wout_stopwords = []\n",
        "    dicc_termns = {}\n",
        "    for i in x:\n",
        "        if TextBlob(i).detect_language() != \"en\":\n",
        "           a= str(TextBlob(i).translate(to = \"en\"))\n",
        "           text_tokens.append(tokenizer.tokenize(a.lower())) #tokenizar y quitar signos de puntuación\n",
        "        else : text_tokens.append(tokenizer.tokenize(i.lower()))\n",
        "    \n",
        "    for i in text_tokens:\n",
        "        wout_stop = []\n",
        "        for word in i:\n",
        "            if word not in stopwords.words('english'): \n",
        "                wout_stop.append(word)\n",
        "                if(word in dicc_termns):#incrementar palabras al diccionario\n",
        "                    dicc_termns[word] = dicc_termns[word] + 1\n",
        "                elif(word not in dicc_termns):#agregar palabras al diccionario        \n",
        "                     dicc_termns[word] = 1\n",
        "        text_tokens_wout_stopwords.append(wout_stop)\n",
        "        \n",
        "    matrix_1 = np.zeros((len(text_tokens_wout_stopwords), len(dicc_termns)))# Pre-allocate matrix\n",
        "    matrix_2 = np.zeros((len(text_tokens_wout_stopwords), len(dicc_termns)))# Pre-allocate matrix\n",
        "    i = 0\n",
        "    for word_termns in dicc_termns: #dicc_termns todos los términos\n",
        "        j = 0\n",
        "        for word_texts in text_tokens_wout_stopwords:#listado de todos los textos\n",
        "            matrix_2[j,i] = word_texts.count(word_termns) \n",
        "            if(word_termns in word_texts): #si está\n",
        "                matrix_1[j, i] = 1 \n",
        "            j = j + 1  \n",
        "        i = i + 1    \n",
        "    return(text_tokens_wout_stopwords,dicc_termns,matrix_1,matrix_2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIQyTmbT8Cul",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "8d92acd6-81df-4d5d-b4b8-bf404bd2b84c"
      },
      "source": [
        "#tokenizacion\n",
        "token_Bow_MTDB = token_bow_mtdb(list_tw)\n",
        "tokenizacion = token_Bow_MTDB[0]\n",
        "i=0\n",
        "for element in tokenizacion:\n",
        "  print(\"El texto tiene \",len(tokenizacion[i]), \"palabras.\")\n",
        "  i=i+1\n",
        "\n",
        "tokenizacion"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El texto tiene  4 palabras.\n",
            "El texto tiene  7 palabras.\n",
            "El texto tiene  7 palabras.\n",
            "El texto tiene  7 palabras.\n",
            "El texto tiene  7 palabras.\n",
            "El texto tiene  7 palabras.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['hello', 'hello', 'hello', 'hello'],\n",
              " ['hi', 'class', 'interesting', 'like', 'program', 'good', 'yet'],\n",
              " ['hi', 'class', 'interesting', 'like', 'plan', 'good', 'yet'],\n",
              " ['hello', 'class', 'interesting', 'like', 'programming', 'still', 'good'],\n",
              " ['hi', 'class', 'interesting', 'like', 'programming', 'good', 'yet'],\n",
              " ['hello', 'race', 'interesting', 'like', 'programming', 'still', 'good']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiR-_I8v9r8A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "8d6c5775-f367-48cc-ae7a-cbfdd0236771"
      },
      "source": [
        "#Bow\n",
        "Bow = token_Bow_MTDB[1]\n",
        "print(len(Bow))\n",
        "Bow"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'class': 4,\n",
              " 'good': 5,\n",
              " 'hello': 6,\n",
              " 'hi': 3,\n",
              " 'interesting': 5,\n",
              " 'like': 5,\n",
              " 'plan': 1,\n",
              " 'program': 1,\n",
              " 'programming': 3,\n",
              " 'race': 1,\n",
              " 'still': 2,\n",
              " 'yet': 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv11-273-Hs-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "e57870c9-3aa7-46fb-c4b6-2950a1e29a72"
      },
      "source": [
        "#Matriz Término Documento (binaria)\n",
        "mat_tdb = token_Bow_MTDB[2]\n",
        "print(mat_tdb.shape)\n",
        "mat_tdb"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6, 12)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
              "       [0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0.],\n",
              "       [1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0.],\n",
              "       [0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0.],\n",
              "       [1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eOqDyiqBQ3C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "9f819397-9c3f-456f-f578-94d8cd987d30"
      },
      "source": [
        "tamano_mat=mat_tdb.shape[0]-1\n",
        "#print(tamano_mat)\n",
        "i=0\n",
        "j=0\n",
        "while i < tamano_mat:\n",
        "  j = i+1\n",
        "  if (i!=j):\n",
        "    while j < tamano_mat:\n",
        "      bin_cos = dot(mat_tdb[i],mat_tdb[j])/(norm(mat_tdb[i])*norm(mat_tdb[j]))\n",
        "      print(f\"El producto escalar entre los textos {i+1} y {j+1} es de :\",round(bin_cos,4))\n",
        "      j=j+1    \n",
        "  i=i+1"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El producto escalar entre los textos 1 y 2 es de : 0.0\n",
            "El producto escalar entre los textos 1 y 3 es de : 0.0\n",
            "El producto escalar entre los textos 1 y 4 es de : 0.378\n",
            "El producto escalar entre los textos 1 y 5 es de : 0.0\n",
            "El producto escalar entre los textos 2 y 3 es de : 0.8571\n",
            "El producto escalar entre los textos 2 y 4 es de : 0.5714\n",
            "El producto escalar entre los textos 2 y 5 es de : 0.8571\n",
            "El producto escalar entre los textos 3 y 4 es de : 0.5714\n",
            "El producto escalar entre los textos 3 y 5 es de : 0.8571\n",
            "El producto escalar entre los textos 4 y 5 es de : 0.7143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8kttc7lH6UJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "9aa6402c-b0e8-4827-fae5-da8554eb16ca"
      },
      "source": [
        "#Matriz Término Documento (frecuencia)\n",
        "mat_tdf = token_Bow_MTDB[3]\n",
        "print(\"Tamaño de la matriz: \",mat_tdf.shape,\"\\n\")\n",
        "mat_tdf"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tamaño de la matirz:  (6, 12) \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
              "       [0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0.],\n",
              "       [1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0.],\n",
              "       [0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0.],\n",
              "       [1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHdYz7iEI5J4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "b1f5be9b-55b6-4f45-c1eb-1de6a5e07d71"
      },
      "source": [
        "tamano_matf=mat_tdf.shape[0]-1\n",
        "#print(tamano_matf)\n",
        "i=0\n",
        "j=0\n",
        "while i < tamano_matf:\n",
        "  j = i+1\n",
        "  if (i!=j):\n",
        "    while j < tamano_matf:\n",
        "      df_cos = dot(mat_tdf[i],mat_tdf[j])/(norm(mat_tdf[i])*norm(mat_tdf[j]))\n",
        "      print(f\"El producto escalar entre los textos {i+1} y {j+1} es de :\",round(df_cos,4))\n",
        "      j=j+1    \n",
        "  i=i+1"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El producto escalar entre los textos 1 y 2 es de : 0.0\n",
            "El producto escalar entre los textos 1 y 3 es de : 0.0\n",
            "El producto escalar entre los textos 1 y 4 es de : 0.378\n",
            "El producto escalar entre los textos 1 y 5 es de : 0.0\n",
            "El producto escalar entre los textos 2 y 3 es de : 0.8571\n",
            "El producto escalar entre los textos 2 y 4 es de : 0.5714\n",
            "El producto escalar entre los textos 2 y 5 es de : 0.8571\n",
            "El producto escalar entre los textos 3 y 4 es de : 0.5714\n",
            "El producto escalar entre los textos 3 y 5 es de : 0.8571\n",
            "El producto escalar entre los textos 4 y 5 es de : 0.7143\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}